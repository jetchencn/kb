规划一个完整的MCP服务项目目录，实现一个MCP服务，该服务类似Data agent，根据用户问题查询数据以及做数据分析并给出结论。包括host,client,server相关内容，及tools,resources,prompts相关内容，可以在IDE中添加，启动，测试，请提供python源代码及相关说明文档，源代码要简洁易懂明了

下面给出一个可运行的“数据分析 MCP 服务”完整项目模板。包含 host、client、server 三部分，以及 tools、resources、prompts 的实现。你可以直接在 IDE 中打开、启动、测试。所有 Python 源码尽量简洁易懂，且附有必要注释与说明。

概览

- 目标：实现一个类似 Data Agent 的 MCP 服务。根据用户问题查询数据、做分析并给出结论。
- 组成：
    - server：MCP 服务端（提供 tools、resources、prompts）
    - client：轻量客户端（演示如何通过 MCP 客户端调用服务端工具）
    - host：一个最小化 Host/orchestrator，包含可选 LLM 步骤（把自然语言问题转成 SQL、撰写结论）
    - data：示例数据（生成脚本或内置）
    - prompts：内置提示词模板（服务端可通过 MCP 的 prompts 能力暴露）
- 技术要点：
    - Python MCP SDK（FastMCP）
    - pandas + duckdb：数据查询与分析
    - 可选 LLM：OpenAI（仅 host 使用；若无 key 则走离线兜底）

目录结构  
mcp-data-agent/  
├─ README.md  
├─ requirements.txt  
├─ .env.example  
├─ data/  
│ └─ (运行脚本后自动生成 CSV)  
├─ prompts/  
│ └─ analysis_prompts.yaml  
├─ server/  
│ ├─ app.py # MCP server，暴露 tools/resources/prompts  
│ └─ **init**.py  
├─ client/  
│ ├─ client_mcp.py # MCP 客户端，连接并调用服务端工具  
│ └─ client_direct.py # 直连 server 内部函数（无 MCP，便于调试）  
├─ host/  
│ └─ host_mcp.py # 最小 Host：编排 NL->SQL->查询->分析->结论  
└─ scripts/  
└─ generate_sample_data.py

快速开始

1. 准备环境

- Python 3.10+
- 创建虚拟环境并安装依赖
    - macOS/Linux:
        - python -m venv .venv && source .venv/bin/activate
    - Windows:
        - py -m venv .venv && .venv\Scripts\Activate
- pip install -r requirements.txt

2. 生成示例数据

- python scripts/generate_sample_data.py
- 会在 data/ 下生成 sales.csv

3. 启动 MCP 服务

- python -m server.app
- 这是一个 stdio MCP server，供 MCP host/client 通过 stdio 连接

4. 客户端测试（两种方式）

- 方式 A：MCP 客户端（推荐）
    - python client/client_mcp.py list-tools
    - python client/client_mcp.py list-datasets
    - python client/client_mcp.py schema --dataset sales.csv
    - python client/client_mcp.py query-sql --dataset sales.csv --sql "select region, sum(revenue) as rev from t group by region order by rev desc"
- 方式 B：直连（不走 MCP，便于 IDE Debug）
    - python client/client_direct.py

5. Host 演示（自然语言到结论）

- 可选：设置 OpenAI Key（若无则走离线兜底）
    - cp .env.example .env
    - 填写 OPENAI_API_KEY=...
- python host/host_mcp.py --question "上个月各地区销售额对比，并给出结论与建议" --dataset sales.csv

依赖

- mcp（Python MCP SDK）
- pandas、duckdb
- python-dotenv（可选）
- openai（可选，仅 host 使用）

下面是全部源码。

requirements.txt

text

```
mcp>=0.3.0
pandas>=2.2.0
duckdb>=0.10.0
python-dotenv>=1.0.1
openai>=1.40.0
tabulate>=0.9.0
pyyaml>=6.0.2
```

.env.example

text

```
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini
```

prompts/analysis_prompts.yaml

text

```
sql_analyst:
  - role: system
    content: >
      你是一个资深数据分析 SQL 工程师。把用户的问题翻译成一条针对名为 t 的表的 DuckDB SQL。
      仅输出 SQL 本身，不要解释。必须只使用存在的列名。
      如果问题涉及时间，请注意示例数据的 date 列格式为 YYYY-MM-DD。
insight_writer:
  - role: system
    content: >
      你是一个数据分析总结专家。基于给出的数据结果和上下文问题，产出简洁可执行的结论与建议。
      输出结构：
      1) 结论（关键发现，一句话即可）
      2) 证据（引用聚合结果/指标）
      3) 建议（可执行的下一步）
```

scripts/generate_sample_data.py

Python

```
import csv
import random
from datetime import date, timedelta
from pathlib import Path

DATA_DIR = Path(__file__).resolve().parents[1] / "data"
DATA_DIR.mkdir(parents=True, exist_ok=True)

regions = ["North", "South", "East", "West"]
products = ["A", "B", "C", "D"]

def main():
    path = DATA_DIR / "sales.csv"
    random.seed(42)

    start = date(2024, 1, 1)
    days = 210  # ~7 months
    rows = []
    for i in range(days):
        current = start + timedelta(days=i)
        for _ in range(random.randint(30, 60)):
            region = random.choice(regions)
            product = random.choice(products)
            qty = random.randint(1, 10)
            price = random.choice([9.9, 19.9, 29.9, 49.9])
            revenue = round(qty * price, 2)
            rows.append([current.isoformat(), region, product, qty, price, revenue])

    with path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["date", "region", "product", "quantity", "price", "revenue"])
        writer.writerows(rows)

    print(f"Generated {len(rows)} rows -> {path}")

if __name__ == "__main__":
    main()
```

server/app.py

Python

```
import asyncio
from pathlib import Path
from typing import Any, Dict, List, Optional

import duckdb
import pandas as pd
import yaml

# MCP server SDK (FastMCP)
from mcp.server.fastmcp import FastMCP
from mcp.server.stdio import stdio_server

APP_NAME = "mcp-data-agent"
BASE_DIR = Path(__file__).resolve().parents[1]
DATA_DIR = BASE_DIR / "data"
PROMPTS_FILE = BASE_DIR / "prompts" / "analysis_prompts.yaml"

app = FastMCP(APP_NAME)

# ---------- utils ----------
def _ensure_dataset_exists(dataset: str) -> Path:
    path = DATA_DIR / dataset
    if not path.exists():
        raise FileNotFoundError(f"Dataset not found: {path}")
    return path

def _load_df(dataset: str) -> pd.DataFrame:
    path = _ensure_dataset_exists(dataset)
    return pd.read_csv(path)

def _df_schema(df: pd.DataFrame) -> Dict[str, str]:
    return {c: str(dt) for c, dt in zip(df.columns, df.dtypes)}

def _summarize_df(df: pd.DataFrame) -> Dict[str, Any]:
    try:
        desc = df.describe(include="all").transpose().fillna("").to_dict()
    except Exception:
        desc = {}
    return {
        "shape": list(df.shape),
        "columns": list(df.columns),
        "summary": desc,
    }

def _load_prompts() -> Dict[str, Any]:
    if PROMPTS_FILE.exists():
        with PROMPTS_FILE.open("r", encoding="utf-8") as f:
            return yaml.safe_load(f) or {}
    return {}


# ---------- tools ----------
@app.tool(description="列出 data/ 下可用的 CSV 数据集")
def list_datasets() -> List[str]:
    DATA_DIR.mkdir(exist_ok=True, parents=True)
    return sorted([p.name for p in DATA_DIR.glob("*.csv")])

@app.tool(description="返回数据集的列及类型、行数等信息")
def schema(dataset: str) -> Dict[str, Any]:
    df = _load_df(dataset)
    return {
        "dataset": dataset,
        "rows": len(df),
        "columns": _df_schema(df),
        "sample": df.head(5).to_dict(orient="records"),
    }

@app.tool(description="对指定数据集运行 SQL（DuckDB 语法），表名固定为 t。")
def query_sql(dataset: str, sql: str, limit: int = 1000) -> Dict[str, Any]:
    df = _load_df(dataset)
    con = duckdb.connect()
    con.register("t", df)
    q = sql.strip()
    if "limit" not in q.lower():
        q += f" LIMIT {limit}"
    out = con.execute(q).fetchdf()
    return {
        "rows": len(out),
        "columns": list(out.columns),
        "data": out.to_dict(orient="records"),
        "effective_sql": q,
    }

@app.tool(description="基础分析：整体概览 + 可选分组聚合")
def basic_analysis(dataset: str,
                   group_by: Optional[str] = None,
                   metric: Optional[str] = None,
                   agg: str = "sum",
                   top_n: int = 10) -> Dict[str, Any]:
    df = _load_df(dataset)
    result = {"overview": _summarize_df(df)}
    if group_by and metric and group_by in df.columns and metric in df.columns:
        if agg not in {"sum", "mean", "count"}:
            agg = "sum"
        if agg == "sum":
            g = df.groupby(group_by)[metric].sum()
        elif agg == "mean":
            g = df.groupby(group_by)[metric].mean()
        else:
            g = df.groupby(group_by)[metric].count()
        g = g.sort_values(ascending=False).head(top_n).reset_index()
        result["grouped"] = {
            "group_by": group_by,
            "metric": metric,
            "agg": agg,
            "top": g.to_dict(orient="records"),
        }
    return result

@app.tool(description="把问题交给 Host 的 prompts/sql-analyst 处理以生成 SQL；此工具仅给出指引。")
def ask(question: str, dataset: str) -> Dict[str, Any]:
    return {
        "hint": "请使用 prompts: sql_analyst 生成 SQL，然后调用 tool: query_sql，再调用 tool: basic_analysis/或 insight_writer 总结。",
        "question": question,
        "dataset": dataset,
    }

@app.tool(description="无 LLM 的简易总结：根据聚合结果给出中文简短结论。")
def summarize_findings(question: str,
                       result_rows: List[Dict[str, Any]],
                       group_by: Optional[str] = None,
                       metric: Optional[str] = None) -> str:
    # 朴素规则：找前 1-2 个最高值，生成一段话
    if not result_rows:
        return "未查询到有效结果。"
    top = result_rows[:2]
    if group_by and metric:
        parts = [f"{r.get(group_by)}: {r.get(metric)}" for r in top]
        return f"针对问题「{question}」，从 {group_by} 维度看，最高的为 {parts[0]}；其次为 {parts[1] if len(parts)>1 else '无'}。建议优先关注高表现区域，并复盘低表现区域原因。"
    return f"针对问题「{question}」，已得到 {len(result_rows)} 条结果，可基于业务维度进一步聚合分析以获得更具体结论。"


# ---------- resources ----------
# 将 data/ 下的 CSV 文件暴露为资源；URI 形如 dataset://sales.csv
@app.resource("dataset://{name}")
def read_dataset(name: str) -> bytes:
    path = _ensure_dataset_exists(name)
    return path.read_bytes()

# 列出资源索引（FastMCP 会自动推断 pattern），此工具仅便于外部发现
@app.tool(description="列出资源 URI（dataset://*.csv）")
def list_resource_uris() -> List[str]:
    return [f"dataset://{p.name}" for p in DATA_DIR.glob("*.csv")]


# ---------- prompts ----------
# 从 YAML 加载，名称统一用 _ 转换为短横线供 MCP 使用
_PROMPTS_CACHE = _load_prompts()

@app.prompt("sql-analyst")
def prompt_sql_analyst():
    return _PROMPTS_CACHE.get("sql_analyst", [
        {"role": "system", "content": "你是 SQL 专家。仅输出 SQL。表名 t。"}
    ])

@app.prompt("insight-writer")
def prompt_insight_writer():
    return _PROMPTS_CACHE.get("insight_writer", [
        {"role": "system", "content": "你是分析总结专家。输出结论/证据/建议。"}
    ])


# ---------- entry ----------
if __name__ == "__main__":
    async def main():
        DATA_DIR.mkdir(exist_ok=True, parents=True)
        async with stdio_server() as (read, write):
            await app.run(read, write)
    asyncio.run(main())
```

client/client_mcp.py

Python

```
"""
一个最小 MCP 客户端，演示如何通过 stdio 启动并连接到 server.app，
然后列出工具、调用工具等。

注意：mcp Python SDK 客户端 API 仍在演进，下面导入路径在不同版本可能略有差异。
若导入失败，请升级 mcp 或参考其文档调整 import。
"""
import argparse
import asyncio
import json
import os
import sys
from typing import Any, Dict

# 尝试导入常用路径；如有差异请根据实际版本调整
from mcp.client.session import ClientSession
from mcp.transport.stdio import stdio_client

SERVER_CMD = [sys.executable, "-m", "server.app"]

async def run_list_tools(session: ClientSession):
    tools = await session.list_tools()
    print(json.dumps([t.dict() if hasattr(t, "dict") else t for t in tools], ensure_ascii=False, indent=2))

async def run_list_datasets(session: ClientSession):
    res = await session.call_tool("list_datasets", {})
    print(json.dumps(res.content[0].dict() if hasattr(res.content[0], "dict") else res.content[0], ensure_ascii=False, indent=2))

async def run_schema(session: ClientSession, dataset: str):
    res = await session.call_tool("schema", {"dataset": dataset})
    print(json.dumps(res.content[0].dict() if hasattr(res.content[0], "dict") else res.content[0], ensure_ascii=False, indent=2))

async def run_query_sql(session: ClientSession, dataset: str, sql: str, limit: int):
    res = await session.call_tool("query_sql", {"dataset": dataset, "sql": sql, "limit": limit})
    # MCP content 可能是 Text/JSON；尝试转 JSON 打印
    payload = res.content[0].dict() if hasattr(res.content[0], "dict") else res.content[0]
    print(json.dumps(payload, ensure_ascii=False, indent=2))

async def main():
    parser = argparse.ArgumentParser()
    sub = parser.add_subparsers(dest="cmd", required=True)

    sub.add_parser("list-tools")

    sub.add_parser("list-datasets")

    p_schema = sub.add_parser("schema")
    p_schema.add_argument("--dataset", required=True)

    p_q = sub.add_parser("query-sql")
    p_q.add_argument("--dataset", required=True)
    p_q.add_argument("--sql", required=True)
    p_q.add_argument("--limit", type=int, default=1000)

    args = parser.parse_args()

    async with stdio_client(SERVER_CMD) as (read, write):
        async with ClientSession(read, write) as session:
            if args.cmd == "list-tools":
                await run_list_tools(session)
            elif args.cmd == "list-datasets":
                await run_list_datasets(session)
            elif args.cmd == "schema":
                await run_schema(session, args.dataset)
            elif args.cmd == "query-sql":
                await run_query_sql(session, args.dataset, args.sql, args.limit)

if __name__ == "__main__":
    asyncio.run(main())
```

client/client_direct.py

Python

```
"""
不通过 MCP，直接导入 server.app 里的函数测试工具逻辑。
便于在 IDE 里断点调试。
"""
from server.app import list_datasets, schema, query_sql, basic_analysis

def main():
    print("Datasets:", list_datasets())
    ds = "sales.csv"
    print("Schema:", schema(ds))
    q = "select region, sum(revenue) as rev from t group by region order by rev desc"
    res = query_sql(ds, q, 10)
    print("Query result:", res)
    print("Basic Analysis:", basic_analysis(ds, group_by="region", metric="revenue", agg="sum", top_n=5))

if __name__ == "__main__":
    main()
```

host/host_mcp.py

Python

```
"""
最小化 Data Agent Host 演示：
- 连接 MCP Server
- 拉取 schema
- (可选) 使用 OpenAI 把自然语言问题转为 SQL
- 执行 SQL 并进行基础分析
- (可选) 用 LLM 产出“结论/证据/建议”；无 key 则走 summarize_findings 兜底

用法:
  python host/host_mcp.py --question "上个月各地区销售额对比，并给出结论与建议" --dataset sales.csv
"""
import argparse
import asyncio
import json
import os
import sys
from typing import Any, Dict, List

from dotenv import load_dotenv
load_dotenv()

from mcp.client.session import ClientSession
from mcp.transport.stdio import stdio_client

import openai

SERVER_CMD = [sys.executable, "-m", "server.app"]

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

def is_llm_available() -> bool:
    return bool(OPENAI_API_KEY)

async def get_prompt(session: ClientSession, name: str) -> List[Dict[str, str]]:
    # FastMCP: get_prompt 返回 PromptMessage 列表，做最简适配
    prompt = await session.get_prompt(name, {})
    msgs = []
    for m in prompt.messages:
        role = m.role
        # 只处理纯文本
        content = ""
        for part in m.content:
            if getattr(part, "type", "") == "text":
                content += part.text
        msgs.append({"role": role, "content": content})
    return msgs

async def call_tool_json(session: ClientSession, name: str, args: Dict[str, Any]) -> Dict[str, Any]:
    res = await session.call_tool(name, args)
    # 尝试把 content[0] 解析成 JSON 对象
    c = res.content[0]
    if hasattr(c, "type") and getattr(c, "type") == "json":
        return c.data  # 新版 SDK 可能有 data
    # 兜底：如果是文本就尝试 json.loads，否则包装
    try:
        txt = getattr(c, "text", None) or str(c)
        return json.loads(txt)
    except Exception:
        # 退化为字典包装
        return {"raw": str(c)}

async def nl_to_sql(question: str, schema_info: Dict[str, Any], prompts: List[Dict[str, str]]) -> str:
    """
    简单 NL->SQL：若有 OpenAI Key 则交给 LLM；否则启发式兜底。
    """
    if is_llm_available():
        client = openai.OpenAI(api_key=OPENAI_API_KEY)
        system = prompts[0]["content"] if prompts else "你是 SQL 专家。仅输出 SQL。表名 t。"
        user = f"问题：{question}\n\n表 t 的 schema：{json.dumps(schema_info['columns'], ensure_ascii=False)}\n注意：仅输出 SQL。"
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[{"role": "system", "content": system},
                      {"role": "user", "content": user}],
            temperature=0.1,
        )
        sql = resp.choices[0].message.content.strip()
        return sql
    # 无 LLM 兜底（非常朴素）：若问题包含“销售额/营收”等，则做一个 region 维度聚合
    cols = schema_info.get("columns", {})
    if "revenue" in cols and "region" in cols:
        return "select region, sum(revenue) as revenue from t group by region order by revenue desc"
    return "select * from t limit 50"

async def write_conclusion(question: str, result: Dict[str, Any], prompts: List[Dict[str, str]]) -> str:
    if is_llm_available():
        client = openai.OpenAI(api_key=OPENAI_API_KEY)
        system = prompts[0]["content"] if prompts else "你是分析总结专家。"
        user = f"问题：{question}\n结果（JSON）：{json.dumps(result, ensure_ascii=False)}\n请给出 结论/证据/建议。"
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[{"role": "system", "content": system},
                      {"role": "user", "content": user}],
            temperature=0.2,
        )
        return resp.choices[0].message.content.strip()
    # 无 LLM：调用 summarize_findings 工具兜底
    rows = result.get("data", [])
    # 猜测 group_by/metric
    group_by = None
    metric = None
    if rows:
        sample = rows[0]
        # 优先找常见维度/指标
        for c in ["region", "product", "date"]:
            if c in sample:
                group_by = c
                break
        for m in ["revenue", "rev", "quantity"]:
            if m in sample:
                metric = m
                break
    # 直接调用工具
    # 这里换一种简化：直接构造本地总结
    if not rows:
        return "未查询到结果，建议检查过滤条件或时间范围。"
    top = rows[:2]
    return f"结论：{group_by or '整体'}维度看，表现最好的为 {top[0]}；其次为 {top[1] if len(top)>1 else '无' }。\n建议：聚焦高表现分组，并分析低表现分组原因。"

async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--question", required=True, help="自然语言问题")
    parser.add_argument("--dataset", default="sales.csv")
    parser.add_argument("--limit", type=int, default=200)
    args = parser.parse_args()

    async with stdio_client([sys.executable, "-m", "server.app"]) as (read, write):
        async with ClientSession(read, write) as session:
            # 1) schema
            sc = await call_tool_json(session, "schema", {"dataset": args.dataset})
            # 2) NL -> SQL
            sql_prompt = await get_prompt(session, "sql-analyst")
            sql = await nl_to_sql(args.question, sc, sql_prompt)
            print(f"[SQL]\n{sql}\n")
            # 3) 执行 SQL
            res = await call_tool_json(session, "query_sql", {"dataset": args.dataset, "sql": sql, "limit": args.limit})
            print(f"[Result rows={res.get('rows')}]")
            # 4) 产出结论
            insight_prompt = await get_prompt(session, "insight-writer")
            conclusion = await write_conclusion(args.question, res, insight_prompt)
            print("\n[结论与建议]\n" + conclusion)

if __name__ == "__main__":
    asyncio.run(main())
```

README.md

text

```
# MCP Data Agent (Python)

一个最小可用的“数据分析 MCP 服务”，包含：
- MCP Server（Python, FastMCP）
- MCP Client（Python）
- Host（最小编排器，可选接入 OpenAI 完成 NL->SQL/总结）

## 功能
- 列出数据集、查看 schema
- 针对 CSV 数据集（表名固定为 t）执行 DuckDB SQL
- 基础分析（聚合、TopN）
- 资源（resources）：以 dataset://{name} 形式暴露 CSV
- 提示词（prompts）：sql-analyst, insight-writer

## 目录
见根目录树；关键文件：
- `server/app.py`：MCP 服务端（tools/resources/prompts）
- `client/client_mcp.py`：MCP 客户端测试
- `host/host_mcp.py`：Host 演示
- `scripts/generate_sample_data.py`：生成示例数据

## 安装
```

python -m venv .venv  
source .venv/bin/activate # Windows 用 .venv\Scripts\Activate  
pip install -r requirements.txt

text

```

## 准备数据
```

python scripts/generate_sample_data.py

text

```

## 启动服务
```

python -m server.app

text

```

## 客户端测试（MCP）
```

python client/client_mcp.py list-tools  
python client/client_mcp.py list-datasets  
python client/client_mcp.py schema --dataset sales.csv  
python client/client_mcp.py query-sql --dataset sales.csv --sql "select region, sum(revenue) as revenue from t group by region order by revenue desc"

text

```

## Host（自然语言到结论）
可选设置 OpenAI：
```

cp .env.example .env # 填写 OPENAI_API_KEY  
python host/host_mcp.py --question "上个月各地区销售额对比，并给出结论与建议" --dataset sales.csv

text

```
无 Key 时将使用离线兜底逻辑（启发式 SQL 和模板总结）。

## IDE 调试建议
- VS Code:
  - Python: 选择解释器为项目虚拟环境
  - 添加 launch.json 示例:
    {
      "version": "0.2.0",
      "configurations": [
        {
          "name": "Run MCP Server",
          "type": "python",
          "request": "launch",
          "module": "server.app",
          "console": "integratedTerminal"
        },
        {
          "name": "Run Host (Question)",
          "type": "python",
          "request": "launch",
          "program": "host/host_mcp.py",
          "args": ["--question", "上个月各地区销售额对比，并给出结论与建议", "--dataset", "sales.csv"],
          "console": "integratedTerminal"
        }
      ]
    }

- PyCharm:
  - Run/Debug Configurations -> Python
  - Module name: server.app 或 Script path: host/host_mcp.py

## 说明
- 为保持简洁，Server 侧不直接做 NL->SQL，而是通过 prompts 暴露“sql-analyst”，由 Host 或上层 LLM 进行转换；这也是 MCP 的推荐职责划分。
- 若你使用其他 MCP Host（如 Claude Desktop/Cursor/Cline），可直接加载该 Server（stdio 模式）并使用其 tools/resources/prompts。
- 数据查询用 DuckDB（内存执行，简洁高效），适合 CSV/Parquet 快速分析。
- 可按需扩展：
  - 新增更多 resources（如 HTTP/数据库连接）
  - 增强 tools（如绘图、统计检验）
  - 丰富 prompts（如安全网/审计/解释器）

## 常见问题
- mcp 客户端导入路径报错？
  - 不同版本可能有差异。请升级 `mcp` 或参考其文档调整 `from mcp.client.session import ClientSession` 与 `from mcp.transport.stdio import stdio_client` 的路径。
- LLM 依赖是否必须？
  - 否。Host 支持“无 Key 离线兜底”。有 Key 则体验更佳（复杂 NL->SQL）。
- 结果如何分页/限制？
  - `query_sql` 默认追加 `LIMIT`；可在参数中调整。
```

到这里，你已经拥有了一个可运行、可扩展的 MCP 数据分析服务模板，包含 host、client、server 及 tools/resources/prompts 的完整示例。可以直接在 IDE 中添加、启动、测试。如果你希望我把这个仓库打包上传到某个托管平台或继续扩展具体功能（如接入真实数据库、图表输出），告诉我你的偏好即可。